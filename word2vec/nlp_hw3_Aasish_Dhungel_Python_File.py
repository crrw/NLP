# -*- coding: utf-8 -*-
"""NLP_HW3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1a8LH2ipQXpZNbxmMX1uLu0SkSJQTY5Vm
"""

import bz2
import gensim
import logging

logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

data_file="news.crawl.bz2"

with bz2.open ('news.crawl.bz2', 'rb') as f:
    for i,line in enumerate (f):
        print(line)
        break

def read_input(input_file):   
    logging.info("reading file {0}...this may take a while".format(input_file))
    
    with bz2.open (input_file, 'rb') as f:
        for i, line in enumerate (f): 

            if (i%10000==0):
                logging.info ("read {0} reviews".format (i))
            yield gensim.utils.simple_preprocess (line)


documents = list (read_input (data_file))
logging.info ("Done reading data file")

model = gensim.models.Word2Vec (documents, size=150, window=5, min_count=2, workers=10, iter=10)

model.wv.similarity('dirty','clean')

model.wv.similarity('big','dirty')

model.wv.similarity('big','large')

model.wv.similarity('big','small')

w1 = ["polite"]
model.wv.most_similar(w1,topn=5)

w1 = ["orange"]
model.wv.most_similar(w1,topn=5)

model2 = gensim.models.Word2Vec (documents, size=50, window=2, min_count=2, workers=10, iter=10)

model2.wv.similarity('dirty','clean')

model2.wv.similarity('big','dirty')

model2.wv.similarity('big','large')

model2.wv.similarity('big','small')

w1 = ["polite"]
model2.wv.most_similar(w1,topn=5)

w1 = ["orange"]
model2.wv.most_similar(w1,topn=5)

